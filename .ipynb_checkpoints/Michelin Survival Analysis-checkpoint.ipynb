{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michelin survivor\n",
    "Michelin-rated restaurants are some of the highest-grossing businesses in the fine-dining industry – a $10 billion market in the U.S. Inversely, losing a Michelin star can make a business less profitable, with up to 40% of these businesses closing within 5 years. In this notebook which I developed at Insight Data Science, I used Yelp reviews to develop a product that forecasts the risk of a restaurant losing a Michelin star and provides actionable insights to restauranteurs and investors on how to improve if their business is deemed at risk. Restaurants have vastly different timelines - from opening, getting Michelin-rated, to closing shop - so wrangling 200k+ Yelp reviews into usable features was a significant challenge to overcome, and vital in broader risk assessment and survival analysis problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "michelin_data    = pd.read_csv('michelin_nyc_stars.csv')\n",
    "michelin_data    = michelin_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Michelin Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_michelin(michelin_data):\n",
    "    #-----------------------------------------------------------\n",
    "    # returns all michelin data in accordance with yelp data for NYC restaurants\n",
    "    #-----------------------------------------------------------\n",
    "    def rename_restos(michelin_data):\n",
    "        michelin_data['name'] = michelin_data['name'].replace(to_replace = {'breslin': 'the-breslin',\n",
    "                                   'cho-dang-gol': 'cho-dang-gol-korean-restaurant',\n",
    "                                    'el-parador': 'el-parador-cafe',\n",
    "                                    'ginza-onodera': 'sushi-ginza-onodera',\n",
    "                                    'good-fork': 'the-good-fork',\n",
    "                                    'grammercy-tavern': 'gramercy-tavern',\n",
    "                                                           'great-ny-noodletown': 'great-ny-noodle-town',\n",
    "                                                            'katzs': 'katzs-delicatessen',\n",
    "                                                            'kurumazushi': 'kuruma-zushi',\n",
    "                                                          'modern' : 'the-modern',\n",
    "                                                           'modern,-the': 'the-modern',\n",
    "                                                          'sevilla': 'sevilla-restaurant',\n",
    "                                                           'spotted-pig' : 'the-spotted-pig',\n",
    "                                                            'gunter-seeger-ny': 'gunter-seeger'})\n",
    "\n",
    "        michelin_data['name'] = (michelin_data['name'].str.replace('é', 'e').str.replace('ë', 'e').str.replace('ü', 'u'))\n",
    "        michelin_data         = michelin_data.drop_duplicates(keep = 'first')\n",
    "        michelin_data         = michelin_data.drop([216,217], axis = 0)\n",
    "        \n",
    "        return michelin_data\n",
    "    \n",
    "    def drop_duplicates(michelin_data):\n",
    "\n",
    "        dupes = michelin_data.name[michelin_data.name.duplicated()]\n",
    "        dupes = michelin_data.loc[michelin_data.name.duplicated(keep = False)].sort_values(by = 'name')\n",
    "\n",
    "        new = pd.DataFrame(columns = dupes.columns)\n",
    "        i = 0\n",
    "        for each in dupes.name.unique():\n",
    "            new.loc[i] = dupes.loc[(dupes.name==each)].sum(axis = 0, numeric_only = True)\n",
    "            i+=1\n",
    "        new['name'] = dupes.name.unique()\n",
    "        michelin_data = michelin_data.drop(dupes.index, axis = 0)\n",
    "        michelin_data = pd.concat([michelin_data,new], axis = 0).reset_index(drop = True)\n",
    "        \n",
    "        return michelin_data\n",
    "    \n",
    "    michelin_data = drop_duplicates(rename_restos(michelin_data))\n",
    "    \n",
    "    return michelin_data\n",
    "\n",
    "michelin_data = process_michelin(michelin_data)\n",
    "michelin_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Yelp review data\n",
    " Process Yelp restaurant names then get TF/IDF weights using NMF factorization with hand-labeled topics\n",
    " \n",
    " #### Start with raw data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "def get_yelp_df():\n",
    "    \n",
    "    #-----------------------------------------------------------\n",
    "    # returns all yelp data and topic weights from tf/idf\n",
    "    # for a specific year of restaurants on the michelin guide\n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "    # V_1 added first 06/10/19\n",
    "    old_data  = pd.read_csv('michelin_yelp_reviews.csv')\n",
    "\n",
    "    # added later\n",
    "    new_data  = pd.read_csv('yelp_data_additional.csv', header = None, names = ['restaurant', 'date', 'rating', 'review'])\n",
    "\n",
    "    yelp_data = pd.concat([new_data, old_data], axis = 0)\n",
    "    yelp_data['restaurant'] = (\n",
    "                                yelp_data['restaurant']\n",
    "                                .str.replace('é', 'e')\n",
    "                                .str.replace('ë', 'e')\n",
    "                                .str.replace('ū', 'u')\n",
    "                                .str.replace('ü', 'u')\n",
    "                                .str.replace('gunter-seeger-ny', 'gunter-seeger')\n",
    "                              )\n",
    "    yelp_data = yelp_data.drop_duplicates(keep = 'first')\n",
    "\n",
    "    yelp_data['review'] = (\n",
    "                                yelp_data.review\n",
    "                               .str.replace('\\\\\\\\xc2', '')\n",
    "                               .str.replace('\\\\\\\\xa0', '')\n",
    "                               .str.replace('\\\\\\xa0', '')\n",
    "                               .str.lower()\n",
    "                               .str.replace('\\d+', '')\n",
    "                               .str.replace(r'[^\\w\\s]+', '')\n",
    "                               .str.replace('cocktails', 'cocktail')\n",
    "                               .str.replace('zzs', '')\n",
    "                               .str.replace('xc', '')\n",
    "                               .str.replace('xa', '')\n",
    "                               .str.replace('zz', '')\n",
    "                               .str.replace('       ', '')\n",
    "                               .str.replace('eellent', 'excellent')\n",
    "                               .str.replace(r'\\bthe\\b', '')\n",
    "                               .str.replace(r'\\band\\b', '')\n",
    "                               .str.replace(r'\\bas\\b', '')\n",
    "                               .str.replace(r'\\bof\\b', '') \n",
    "                            )\n",
    "\n",
    "    yelp_data['date']   = pd.to_datetime(yelp_data.date.str.replace('Updatedreview', ''))\n",
    "    \n",
    "    return yelp_data\n",
    "\n",
    "raw_yelp_data      = get_yelp_df()\n",
    "raw_yelp_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process weights and add the processed weights to the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_tf_idf_weights(yelp_data):\n",
    "    #-----------------------------------------------------------\n",
    "    # returns yelp data frame with added TF/IDF weights for each review\n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "    yelp_df = yelp_data.copy()\n",
    "    \n",
    "    topic_labels = ['access',  'service', 'menu', 'value']\n",
    "    \n",
    "    def replace_words(text, dicty):\n",
    "            for i,j in dicty.items():\n",
    "                text = text.replace(i,j)\n",
    "                \n",
    "            return text\n",
    "\n",
    "    def display_topics(model, feature_names, num_topics, no_top_words):\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            if topic_idx < num_topics:\n",
    "                print(\"{:11}\".format(\"Topic %d:\" %(topic_idx)), end='')\n",
    "                print(\", \".join(['{:04.3f}*'.format(topic[i])+feature_names[i] \\\n",
    "                                 for i in topic.argsort()[:-no_top_words-1:-1]]))\n",
    "                \n",
    "    def threshold(number):\n",
    "            if abs(number) > .25:\n",
    "                return(int(1)) * np.sign(number)\n",
    "            else:\n",
    "                return(int(0))\n",
    "            \n",
    "    def get_raw_weights():       \n",
    "        \n",
    "        extra_words = ['ve', 'like', 'got', 'just',\n",
    "                           'don', 'really', 'said', 'told', 'ok',\n",
    "                           'came', 'went', 'did', 'didn', 'good', 'momofuku', 'peter', 'luger' ,'lugers',\n",
    "                      'katzs']\n",
    "        \n",
    "        stop_words  = text.ENGLISH_STOP_WORDS.union(extra_words)\n",
    "        \n",
    "        tfidf       = TfidfVectorizer  (\n",
    "            \n",
    "                                    stop_words=stop_words,\n",
    "                                    min_df=10, \n",
    "                                    max_df=0.5,\n",
    "                                    ngram_range=(1,1), \n",
    "                                    token_pattern='[a-z][a-z]+'\n",
    "            \n",
    "                                        )\n",
    "        \n",
    "        dicty       = {'noodles' : 'noodle', 'dishes': 'dish',\n",
    "                     'buns': 'bun', 'asked' : 'ask',\n",
    "                     'pieces' :'piece', 'burgers' : 'burger' ,\n",
    "                    'minutes' : 'minute', 'orders' : 'order', 'waffles' :'waffle'}\n",
    "\n",
    "\n",
    "        def fit_reviews():\n",
    "            reviews_processed   = [replace_words(w, dicty) for w in yelp_data.review]\n",
    "            review_vectors      = tfidf.fit_transform(reviews_processed )\n",
    "            num_topics          = 10\n",
    "            nmf_reviews         = NMF(n_components=num_topics)\n",
    "            topic_weights       = nmf_reviews.fit_transform(review_vectors)\n",
    "            no_top_words        = 6\n",
    "            print('Top topics + words for all reviews')\n",
    "            print('-'*39)\n",
    "            display_topics(nmf_reviews, tfidf.get_feature_names(), num_topics, no_top_words)\n",
    "            \n",
    "            return topic_weights\n",
    "        \n",
    "        \n",
    "        topic_weights = fit_reviews()\n",
    "        \n",
    "        return topic_weights\n",
    "    \n",
    "    def process_weights(topic_weights):\n",
    "        \n",
    "        topic_weights_df            = pd.DataFrame(normalize(topic_weights[:,[0,4,8,9]], norm='l1'), \n",
    "                                                   columns = topic_labels)\n",
    "\n",
    "        topic_weights_df['menu']    = topic_weights_df['menu']\n",
    "        neg_ind                     = yelp_df.loc[yelp_df['rating']<=3].index.values\n",
    "        \n",
    "        # assign polarities based on # stars in review\n",
    "        for each in topic_labels:\n",
    "            topic_weights_df[each].loc[neg_ind] =  -1* topic_weights_df[each].loc[neg_ind]\n",
    "            topic_weights_df[each]              =  topic_weights_df[each].apply(threshold).astype(int)\n",
    "            \n",
    "        return topic_weights_df\n",
    "\n",
    "    raw_weights       = get_raw_weights()\n",
    "    topic_weights_df  = process_weights(raw_weights)\n",
    "    topic_weights_df  = topic_weights_df.reset_index(drop = True)\n",
    "    yelp_df           = yelp_df.reset_index(drop = True)\n",
    "    new_yelp_data     = pd.concat([yelp_df, topic_weights_df[topic_labels]], axis = 1)\n",
    "    \n",
    "    return new_yelp_data\n",
    "\n",
    "yelp_data          = get_tf_idf_weights(raw_yelp_data)\n",
    "yelp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "* My approach is to process the rating data and the review weights as distributions rather than time-series to obtain static features (such as the mean, median, kurtosis, slope over time, etc...) as predictors of Michelin evaluation outcomes for any given year.\n",
    "\n",
    "* Also included: time since first yelp review, length of review, # of michelin stars\n",
    "\n",
    "* This approach would simplify the model into a binary classificaiton problem, where 1 is losing a star, and 0 is not losing a star, without a temporal time-series component. \n",
    "\n",
    "* In my first pass, my approach was to use each year's yelp review data – including rating distributions and tf/idf weights – as isolated incidents to input into my feature dataframe, resampling each year's data into quarters (Q1-Q4) along with the year's total statistics as rows in my feature set.\n",
    "\n",
    "* This initial approach  did not yield good Logistic regression model performance ( ROC AUC < .6) - suggesting  that Michelin evaluations in any given year may not be a linear approximation of latent signal in Yelp reviews.\n",
    "\n",
    "* Rather than abandoning the hypothesis that Michelin outcomes are a linear funciton of Yelp signal, I realized that cumulatively adding features over the course of each restaurant's existence was a better approach because the model would be able to examine longer-term dynamics for each restaurant.\n",
    "\n",
    "* The following feature enginnering section creates a training set that enables predicting the Michelin evaluation outcome of year (Y) – the evaluation set –  by (i) extracting all the restaurants that were on the Michelin guide in any given year (y), (ii) processing the distribution of ratings  and topic weights into summary statistics for each restaurant for each quarter of it's existence on Yelp, as well as the overall summary statistics as individual feautres, with the predicted outcome of whether it lost a star (1 - lost, 0 - not lost) in year y + 1.\n",
    "\n",
    "* The training set contains the features and outcomes of years 2008 (the first year of Michelin in NYC) through Y-1 \n",
    "\n",
    "* The holdout set contains the features and outcomes of year Y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from scipy.stats import linregress, kurtosis, skew\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def trend(series):\n",
    "    #-----------------------------------------------------------\n",
    "    # Extracts the slope over time for any statistic in feature set\n",
    "    #-----------------------------------------------------------\n",
    "    try:\n",
    "        slope                     = linregress(range(0,len(series)), series)[0]\n",
    "    except:\n",
    "        slope = 0\n",
    "    return slope\n",
    "\n",
    "def count_char(series):\n",
    "    #-----------------------------------------------------------\n",
    "    # Counts the number of elements in a review string\n",
    "    #-----------------------------------------------------------\n",
    "    num_char = 0\n",
    "    for each in series:\n",
    "        num_char += len(each)\n",
    "    try:\n",
    "        count =  num_char /len(series)\n",
    "    except:\n",
    "        count = 0\n",
    "    return count\n",
    "\n",
    "\n",
    "def scale_X(X):\n",
    "    #-----------------------------------------------------------\n",
    "    # min max scales the features to be usable for model\n",
    "    #-----------------------------------------------------------\n",
    "    scaler  = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(X)\n",
    "    return pd.DataFrame(scaler.transform(X), columns = X.columns, index = X.index).fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get subset data of all yelp data and michelin outcomes for a specific year\n",
    "Using all data up untill Y-1 as training, outcome for year Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_subset_dict(predict_year, michelin_data, yelp_data):\n",
    "    #-----------------------------------------------------------\n",
    "    # returns dict of pandas dataframes corresponding to yelp data and michelin data\n",
    "    # for all data up to specific year of restaurants on the michelin guide\n",
    "    # cutoff of data is 100 days before outcomes announced, usually November\n",
    "    #-----------------------------------------------------------\n",
    "    evaluation_cutoff = 100 # in days\n",
    "    year_data         = dict()\n",
    "\n",
    "    next_year        = 'stars_{}'.format(str(predict_year))\n",
    "    this_year        = 'stars_{}'.format(str(predict_year-1))\n",
    "    michelin_subset  = michelin_data.loc[michelin_data[this_year] != 0]\n",
    "    michelin_subset  = michelin_subset.set_index('name', drop = True)\n",
    "    michelin_subset  = michelin_subset.loc[list(set(michelin_subset.index) & set(yelp_data.restaurant.unique()))]\n",
    "\n",
    "    try:\n",
    "        target = pd.Series((michelin_subset[next_year] - michelin_subset[this_year]) < 0, index = michelin_subset.index).astype(int)\n",
    "    except:\n",
    "        target = []\n",
    "\n",
    "    def check_if_subset(restaurant):\n",
    "        return restaurant in michelin_subset.index\n",
    "\n",
    "    yelp_subset_index = yelp_data.restaurant.apply(check_if_subset)\n",
    "    subset_data  = yelp_data.loc[yelp_subset_index]\n",
    "\n",
    "    subset_data  = subset_data.loc[(subset_data.date < dt.datetime(predict_year, 1, 1)\n",
    "                                   -  dt.timedelta(days = evaluation_cutoff)) ]\n",
    "    \n",
    "    year_data['michelin'] = michelin_subset\n",
    "    year_data['target']   = target\n",
    "    year_data['yelp']     = subset_data\n",
    "\n",
    "    return year_data\n",
    "\n",
    "predict_year = 2018\n",
    "year_data = get_subset_dict(predict_year , michelin_data, yelp_data)\n",
    "year_data['yelp'].head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering pipeline to create training set and holdout set for predicting outcomes in year Y\n",
    "* Start with one restaurant's data\n",
    "\n",
    "* Include all divisions 1-4 of restaurant's existence on Yelp until the cutoff date (100 days before evaluation announcment)\n",
    "\n",
    "* Include total summary statistics (avg of 4 quarters) as separate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_restaurant_yelp_data(restaurant, data_df, predict_year):\n",
    "    \n",
    "    #-----------------------------------------------------------\n",
    "    # returns dict of michelin data and yelp data for a specific restaurant, for a specific outcome year\n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "    yelp_df          = data_df['yelp']\n",
    "    michelin_df      = data_df['michelin']\n",
    "    resto_df         = yelp_df.loc[yelp_df.restaurant == restaurant]\n",
    "    \n",
    "    def make_div_features(restaurant_df, predict_year):\n",
    "        \n",
    "        #-----------------------------------------------------------\n",
    "        # resamples raw data into quarter chunks, processes into sumamr statistics\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        rating_cols = ['slope', 'mean', 'kurtosis', 'skew', 'median', 'std',\n",
    "                    'var', 'number_reviews', 'avg_length_reviews']\n",
    "        review_cols = ['menu', 'service', 'value', 'access']\n",
    "\n",
    "        div_df   = pd.DataFrame(columns = review_cols + rating_cols)\n",
    "        total_df = div_df.copy()\n",
    "        \n",
    "        # make relative time series\n",
    "        restaurant_df.index            = pd.date_range(start = '2000-01-01', end =  '2000-12-31', periods=restaurant_df.shape[0])\n",
    "\n",
    "        #resample four times for different measures\n",
    "        div_df['mean']                 = restaurant_df.rating.resample('4M').mean().reset_index(drop = True)\n",
    "        div_df['slope']                = restaurant_df.rating.resample('4M').apply(trend).reset_index(drop = True)\n",
    "        div_df['kurtosis']             = restaurant_df.rating.resample('4M').apply(kurtosis).reset_index(drop = True)\n",
    "        div_df['std']                  = restaurant_df.rating.resample('4M').apply(np.std).reset_index(drop = True)\n",
    "        div_df['var']                  = restaurant_df.rating.resample('4M').apply(np.std).reset_index(drop = True) ** 2\n",
    "        div_df['median']               = restaurant_df.rating.resample('4M').apply(np.median).reset_index(drop = True)\n",
    "        div_df['skew']                 = restaurant_df.rating.resample('4M').apply(skew).reset_index(drop = True)\n",
    "        div_df['number_reviews']       = restaurant_df.rating.resample('4M').apply(len).reset_index(drop = True)\n",
    "        div_df['avg_length_reviews']   = restaurant_df.review.resample('4M').apply(count_char).reset_index(drop = True)\n",
    "        total_df['mean']               = [restaurant_df.rating.mean(), restaurant_df.rating.mean()]\n",
    "        total_df['slope']              = [trend(restaurant_df.rating),trend(restaurant_df.rating)]\n",
    "        total_df['kurtosis']           = [restaurant_df.rating.kurtosis(), restaurant_df.rating.kurtosis()]\n",
    "        total_df['std']                = [np.std(restaurant_df.rating), np.std(restaurant_df.rating)]\n",
    "        total_df['var']                = [np.std(restaurant_df.rating) ** 2, np.std(restaurant_df.rating) ** 2]\n",
    "        total_df['median']             = [np.median(restaurant_df.rating), np.median(restaurant_df.rating)]\n",
    "        total_df['skew']               = [skew(restaurant_df.rating), skew(restaurant_df.rating)]\n",
    "        total_df['number_reviews']     = [len(restaurant_df.rating), len(restaurant_df.rating)]\n",
    "        total_df['avg_length_reviews'] = [np.sum(restaurant_df.review.apply(len))/restaurant_df.shape[0], np.sum(restaurant_df.review.apply(len))/restaurant_df.shape[0]\n",
    "                                         ]\n",
    "        def process_review_weights_data():\n",
    "            for each in review_cols:\n",
    "                div_df[each]   = restaurant_df[each].resample('4M').mean().reset_index(drop = True)\n",
    "                total_df[each] = restaurant_df[each].mean()\n",
    "            return div_df, total_df\n",
    "        \n",
    "        div_df, total_df = process_review_weights_data()\n",
    "        \n",
    "        features = dict()\n",
    "        features['divs']  = div_df\n",
    "        features['total'] = total_df\n",
    "        features['total']['first_review']  = [(\n",
    "                                                (dt.datetime(predict_year, 1, 1)  -  \n",
    "                                                dt.timedelta(days = 100)) - min(restaurant_df['date'])\n",
    "                                          ).days, 0]\n",
    "\n",
    "        return features\n",
    "\n",
    "    return  make_div_features(resto_df, predict_year)\n",
    "\n",
    "boulud_2010_example = process_restaurant_yelp_data('cafe-boulud', year_data, predict_year)\n",
    "boulud_2010_example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all features X and all outcomes y for a given year Y\n",
    "Get dataframes including all features Including all restaurants on the michelin in year Y -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_features_and_predicted(data_df, predict_year):\n",
    "    \n",
    "    #-----------------------------------------------------------\n",
    "    # creates a sample of training or holdout data for one year, including \n",
    "    # features and target outcomes\n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "    measures = ['slope', 'mean', 'kurtosis', 'skew', 'median', 'std',\n",
    "                    'var', 'number_reviews', 'avg_length_reviews', \n",
    "                    'menu', 'service', 'value', 'access']\n",
    "\n",
    "    def get_feature_columns():\n",
    "        columns  = ['name', 'first_review', 'n_stars']\n",
    "\n",
    "        for measure in measures:\n",
    "            for div in range(0,5):\n",
    "                if div < 4:\n",
    "                    columns.append('div{}_{}'.format(div+1, measure))\n",
    "                else:\n",
    "                    columns.append('total_{}'.format(measure))\n",
    "        return columns  \n",
    "\n",
    "    def make_feature_df():\n",
    "        cols              = get_feature_columns()\n",
    "        feature_df        = pd.DataFrame(columns = cols, index = data_df['yelp'].restaurant.unique())\n",
    "        ndivs             = 5\n",
    "        for resto in data_df['yelp'].restaurant.unique():\n",
    "\n",
    "            rating_features     = process_restaurant_yelp_data(resto, data_df, predict_year)\n",
    "            row                 = dict()\n",
    "            row['name']         = resto\n",
    "            row['first_review'] = rating_features['total']['first_review'].iloc[0]\n",
    "            row['n_stars']      = data_df['michelin'].loc[resto]['stars_{}'.format(str(predict_year-1))]\n",
    "            \n",
    "            for measure in measures:\n",
    "                for div in range(0,ndivs):\n",
    "                    if div+1 < ndivs:\n",
    "                        try:\n",
    "                            row['div{}_{}'.format(div+1, measure)]  = rating_features['divs'][measure][div]\n",
    "                        except:\n",
    "                             row['div{}_{}'.format(div+1, measure)] = rating_features['divs'][measure][0]\n",
    "                    else:\n",
    "                        row['total_{}'.format(measure)] = rating_features['total'][measure].loc[0]\n",
    "\n",
    "                    feature_df.loc[resto] = row\n",
    "                    \n",
    "        return feature_df\n",
    "    \n",
    "    def make_features_and_targets_df():\n",
    "        feature_set = make_feature_df()\n",
    "        feature_set = feature_set.fillna(feature_set.mean(axis = 0))\n",
    "        target      = pd.DataFrame(data_df['target'], columns = ['target'])\n",
    "        target['name'] = target.index\n",
    "        target      = target.reset_index(drop = True)\n",
    "        full_data   = feature_set.merge(target, how = 'left', on = 'name')\n",
    "        full_data   = full_data.set_index('name', drop = True)\n",
    "        y           = full_data.target\n",
    "        X           = full_data.drop(columns = 'target')\n",
    "        X           = X.fillna(X.mean())\n",
    "        X           = scale_X(X)\n",
    "        X           = pd.get_dummies(X, columns = ['n_stars'], drop_first= True, prefix = 'michelin_')\n",
    "        return X,y\n",
    "\n",
    "    return make_features_and_targets_df()\n",
    "\n",
    "year_X, year_y = get_features_and_predicted(year_data, predict_year)\n",
    "year_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize correlations accross the feature space\n",
    "Topic scores seem to be correlated, while rating summary stats sem to be a separate entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(32, 32))\n",
    "corr = X_2010.corr(method = 'pearson')\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(corr, vmax=1, square=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now make training set consisting of outcomes and features for all years preceding year Y (in this case 2010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_and_holdout(predict_year, michelin_data, yelp_data):\n",
    "        \n",
    "    #-----------------------------------------------------------\n",
    "    # combines all data from years before predict_year\n",
    "    # summarizes into train['x'], holdout['x'] ---> FEATURES\n",
    "    # train['y'], holdout['y'] ----> targets\n",
    "    #-----------------------------------------------------------\n",
    "    train                      = {'X' : pd.DataFrame(), 'y': pd.Series()}\n",
    "    holdout                    = dict()\n",
    "\n",
    "    predict_year_dict          = get_subset_dict(predict_year, michelin_data, yelp_data)\n",
    "    holdout['X'], holdout['y'] = get_features_and_predicted(predict_year_dict, predict_year)\n",
    "\n",
    "\n",
    "    for year in range(2008, predict_year):\n",
    "        if year == 2011:\n",
    "            continue \n",
    "        year_dict                        = dict()\n",
    "        year_dict                        =  get_subset_dict(year, michelin_data, yelp_data)#.drop('food_quality'4))\n",
    "        year_dict['X'], year_dict['y']   = get_features_and_predicted(year_dict, year)\n",
    "        train['X']                       =  pd.concat([train['X'], year_dict['X']], axis = 0)\n",
    "        train['y']                       =  pd.concat([train['y'], year_dict['y']], axis = 0)\n",
    "    return train, holdout\n",
    "\n",
    "train, holdout = get_train_and_holdout(predict_year, michelin_data, yelp_data)\n",
    "train['X'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to evaluate the linear model\n",
    "Train on years 2008 through Y - 1 (with cross-validation), evaluate on outcomes for year Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "def plot_roc(probs, holdout, title = '', save = False):\n",
    "    \n",
    "    #-----------------------------------------------------------\n",
    "    # plots ROC curve for model performance\n",
    "    #-----------------------------------------------------------\n",
    "    import matplotlib\n",
    "    font = {'family' : 'helvetica',\n",
    "            'weight': 'normal',\n",
    "        'size'   : 12, \n",
    "        }\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(holdout, probs)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    fig = plt.figure(figsize = (7,7))\n",
    "    plt.title(title + '\\nReceiver Operating Characteristic')\n",
    "    plt.grid(b = None)\n",
    "    plt.plot(fpr, tpr, 'slateblue',label = 'AUC = %0.2f' % roc_auc, linewidth = 3)\n",
    "    plt.grid(False)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'--', linewidth = 3, color = 'silver' )\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.tight_layout()\n",
    "    if save is True:\n",
    "        plt.savefig(title + 'roc.png', transparent = True, dpi = 300)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def run_model(train, holdout, cols, clf = LogisticRegression(C = .05), thresh = .5, plot_on = True, title = ''):\n",
    "        \n",
    "    #-----------------------------------------------------------\n",
    "    # runs sklearn LogsticRegression model on train and holdout set, outputs CV score and plots ROC\n",
    "    #-----------------------------------------------------------\n",
    "\n",
    "    clf.fit(train['X'][cols], train['y'])\n",
    "    predicted = clf.predict(holdout['X'][cols])\n",
    "    probs = clf.predict_proba(holdout['X'][cols])\n",
    "    probs = probs[:,1]\n",
    "#     probs[probs > thresh] = 0\n",
    "#     probs = np.where(probs > thresh, 1, 0)\n",
    "\n",
    "    print('Mean Accuracy: %0.2f' % metrics.accuracy_score(holdout['y'], predicted) )\n",
    "    print('Classification Report: \\n', metrics.classification_report(holdout['y'], predicted))\n",
    "    print('Confusion Matrix: \\n', metrics.confusion_matrix(holdout['y'], predicted))\n",
    "    score = cross_val_score(clf, train['X'], train['y'], cv = 10, scoring= 'roc_auc').mean()\n",
    "    print('Cross validation score: {}'.format(score))\n",
    "\n",
    "    plot_roc(probs, holdout['y'], title)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "\n",
    "clf = run_model(train, holdout, cols = train['X'].columns.values, title = f'{str(predict_year)} Model Performance')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling the minority class in the training set should help with model perforamnce\n",
    "We can use the SMOTE algorithm, which uses K-means to generate artificial data points to draw from – bootstrapping model classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resampled(X,y):\n",
    "    #-----------------------------------------------------------\n",
    "    # Uses SMOTE algorithm to oversample minority (LOSES michelin star)\n",
    "    # class. Helpful to improve model performance.\n",
    "    #-----------------------------------------------------------\n",
    "    sm = SMOTE()\n",
    "    X_new, y_new = sm.fit_sample(X, y)\n",
    "\n",
    "    return  pd.DataFrame(X_new, columns = X.columns), pd.Series(y_new)\n",
    "\n",
    "resampled_train = dict()\n",
    "resampled_train['X'], resampled_train['y'] = make_resampled(train['X'], train['y'])\n",
    "clf = run_model(resampled_train, holdout, cols = train['X'].columns.values, title = f'{str(predict_year)} Model Performance')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and parameter tuning\n",
    "#### Create a pipeline to find ideal parameters with feature reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def feature_reduction_pipeline(train, holdout):\n",
    "    # Define a pipeline to search for the best combination of PCA truncation\n",
    "    # and classifier regularization.\n",
    "    logistic = LogisticRegression()\n",
    "    pca = PCA()\n",
    "    \n",
    "    def make_grid_search_pipeline():\n",
    "        pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "\n",
    "\n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {\n",
    "            'pca__n_components': [5, 20, 30, 40, 50, 64],\n",
    "            'logistic__C' : [.01, .05, 1, 5, 10, 100]\n",
    "        }\n",
    "        search = GridSearchCV(pipe, param_grid, iid=False, cv=10)\n",
    "        search.fit(resampled_train['X'], resampled_train['y'])\n",
    "        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "        print(search.best_params_)\n",
    "        return search\n",
    "\n",
    "    # Plot the PCA spectrum\n",
    "    def plot_outcomes():\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "        ax0.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "        ax0.set_ylabel('PCA explained variance')\n",
    "\n",
    "        ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "                    linestyle=':', label='n_components chosen')\n",
    "        ax0.legend(prop=dict(size=12))\n",
    "\n",
    "        # For each number of components, find the best classifier results\n",
    "        results = pd.DataFrame(search.cv_results_)\n",
    "        components_col = 'param_pca__n_components'\n",
    "        best_clfs = results.groupby(components_col).apply(\n",
    "            lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "        best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "                       legend=False, ax=ax1)\n",
    "        ax1.set_ylabel('Classification accuracy (val)')\n",
    "        ax1.set_xlabel('n_components')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    pca.fit(train['X'])\n",
    "    search = make_grid_search_pipeline()\n",
    "    plot_outcomes()\n",
    "    \n",
    "    def evaluate_reduced_data():\n",
    "        train_component   = dict()\n",
    "        holdout_component = dict()\n",
    "        train_component['X']    = pd.DataFrame(pca.transform(train['X'])[:,:search.best_params_['pca__n_components']], columns = None)\n",
    "        train_component['y']    = resampled_train['y']\n",
    "        holdout_component['X']  = pd.DataFrame(pca.transform(holdout['X'])[:,: search.best_params_['pca__n_components']] , columns = None)\n",
    "        holdout_component['y']  = holdout['y']\n",
    "\n",
    "        clf = run_model(train_component, \n",
    "                holdout_component, \n",
    "                cols = train_component['X'].columns, \n",
    "                title = f'{str(predict_year)} Model Performance')\n",
    "    \n",
    "        return train_component, holdout_component\n",
    "    \n",
    "    return evaluate_reduced_data()\n",
    "    \n",
    "feature_reduction_pipeline(resampled_train, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See if recursive feature selection improves model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "def get_rfecv_features(train, clf = LogisticRegression()):\n",
    "    rfecv = RFECV(estimator=clf, step=1, cv=10,\n",
    "                  scoring='roc_auc', min_features_to_select = 10)\n",
    "\n",
    "    rfecv.fit(train['X'], train['y'])\n",
    "    selected_cols = train['X'].columns[rfecv.support_]\n",
    "\n",
    "    print(\"Optimal number of features :{}\".format(rfecv.n_features_) )\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "    return selected_cols\n",
    "\n",
    "selected_features = get_rfecv_features(resampled_train, clf = LogisticRegression(C = .1))\n",
    "clf = run_model(resampled_train, holdout, cols = selected_features, title = f'{str(predict_year)} Model Performance')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and holdout data for a specific year\n",
    "Holdout data corresponds to all the yelp data up to 1 year + 100 days before 1/1 of the predicted year, holdout corresponds to all the data including up to 100 days before the predicted year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_year   = 2019\n",
    "train, holdout = get_train_and_holdout(predict_year, michelin_data, yelp_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a baseline model with logsitc regression to evaluate Michelin survival in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss = 'log')\n",
    "clf = run_model(train, holdout, cols = train['X'].columns.values,  clf = clf)\n",
    "score = cross_val_score(clf, train['X'], train['y'], cv = 10, scoring= 'roc_auc').mean()\n",
    "print('Cross validation score: {}'.format(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample to balanced classes using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resampled_train = {'X' : pd.DataFrame(), 'y': pd.Series()}\n",
    "resampled_train['X'], resampled_train['y'] = make_resampled(train['X'], train['y'])\n",
    "\n",
    "clf = SGDClassifier(loss = 'log', penalty = 'elasticnet')\n",
    "clf = run_model(resampled_train, resampled_train, cols = resampled_train['X'].columns,  clf = clf)\n",
    "\n",
    "score = cross_val_score(clf, resampled_train['X'], resampled_train['y'], cv = 10, scoring= 'roc_auc').mean()\n",
    "print('Cross validation score: {}'.format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coefs_abs = pd.Series(clf.coef_[0], index = train['X'].columns).sort_values(ascending = True)\n",
    "coefs_abs.plot.barh(figsize = (20,20))\n",
    "plt.savefig('all_importance.png', transparent = False, dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss = 'log')\n",
    "clf = run_model(resampled_train, holdout, cols = resampled_train['X'].columns,  clf = clf, title = 'Michelin Survivor Model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "selected_cols = get_rfecv_features(resampled_train)\n",
    "clf = run_model(resampled_train, holdout,   cols = selected_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore just rating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_cols = list()\n",
    "topics = ['wait time', 'service', 'value', 'menu']\n",
    "for each in train['X'].columns:\n",
    "    for topic in topics:\n",
    "        if topic in each:\n",
    "            my_cols.append(each)\n",
    "no_text = dict()            \n",
    "no_text['X'] = train['X'].drop(my_cols, axis = 1)\n",
    "no_text['y'] = train['y'].copy()\n",
    "\n",
    "run_model(no_text, holdout, cols = no_text['X'].columns)\n",
    "\n",
    "resampled_train = {'X' : pd.DataFrame(), 'y': pd.Series()}\n",
    "resampled_train['X'], resampled_train['y'] = make_resampled(no_text['X'],no_text['y'])\n",
    "\n",
    "clf = LogisticRegression(C = .05)\n",
    "clf = run_model(resampled_train, holdout, cols = resampled_train['X'].columns,  title ='No text')\n",
    "coefs_abs = pd.Series(clf.coef_[0], index = resampled_train['X'].columns).sort_values(ascending = True)\n",
    "coefs_abs.plot.barh(figsize = (20,20))\n",
    "plt.savefig('rating_importance.png', transparent = True, dpi = 300)\n",
    "\n",
    "score = cross_val_score(clf, resampled_train['X'], resampled_train['y'], cv = 10, scoring= 'roc_auc').mean()\n",
    "print('Cross validation score: {}'.format(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore only text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = dict()            \n",
    "text['X'] = train['X'][my_cols]\n",
    "text['y'] = train['y'].copy()\n",
    "\n",
    "run_model(text, holdout, cols = text['X'].columns)\n",
    "\n",
    "resampled_train = {'X' : pd.DataFrame(), 'y': pd.Series()}\n",
    "resampled_train['X'], resampled_train['y'] = make_resampled(text['X'],text['y'])\n",
    "\n",
    "clf = LogisticRegression(C = .01)\n",
    "clf = run_model(resampled_train, holdout, cols = resampled_train['X'].columns,   title = 'Only text')\n",
    "coefs_abs = pd.Series(clf.coef_[0], index = resampled_train['X'].columns).sort_values(ascending = True)\n",
    "coefs_abs.plot.barh(figsize = (20,20))\n",
    "plt.savefig('text_importance.png', transparent = True, dpi = 300)\n",
    "\n",
    "\n",
    "score = cross_val_score(clf, resampled_train['X'], resampled_train['y'], cv = 10, scoring= 'roc_auc').mean()\n",
    "print('Cross validation score: {}'.format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate insights for at-risk restaurants for each year\n",
    "\n",
    "Outputs csv for web-app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "predict_years   = [2017,2018]\n",
    "\n",
    "def produce_results(predict_year, michelin_data, yelp_data, thresh = .5):\n",
    "    train, holdout = get_train_and_holdout(predict_year, michelin_data, yelp_data)\n",
    "    df = holdout['X'].copy()\n",
    "    resampled_train = {'X' : pd.DataFrame(), 'y': pd.Series()}\n",
    "    resampled_train['X'], resampled_train['y'] = make_resampled(train['X'], train['y'])\n",
    "\n",
    "    clf  = LogisticRegression(C = .05, random_state=0)\n",
    "    clf.fit(resampled_train['X'], resampled_train['y'])\n",
    "\n",
    "    probs = clf.predict_proba(holdout['X'])[:,1]\n",
    "    \n",
    "    thresh_probs = probs\n",
    "#     thresh_probs[thresh_probs < thresh] = 0\n",
    "    if predict_year != 2020:\n",
    "        plot_roc(thresh_probs, holdout['y'], title = str(predict_year))\n",
    "    \n",
    "    at_risk = thresh_probs\n",
    "    at_risk[at_risk > thresh] = 1\n",
    "    at_risk[at_risk < thresh] = 0\n",
    "    \n",
    "    probs_pct = minmax_scale(probs, feature_range = (.01, .99))\n",
    "    probs_pct = pd.Series(probs_pct, index = holdout['X'].index)\n",
    "    \n",
    "    \n",
    "    like             = 1 - minmax_scale(clf.predict_proba(holdout['X'])[:,1], feature_range=(0,1) )\n",
    "    df['security']   = like\n",
    "    like[like>.5]    = 0\n",
    "    like[like!=0]    = 1\n",
    "    \n",
    "    df['at_risk']    = like.astype(int)\n",
    "    df['actual']     = holdout['y']\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "data_dict = dict()\n",
    "for predict_year in predict_years:\n",
    "    data_dict[predict_year] = produce_results(predict_year, michelin_data, yelp_data)\n",
    "    data_dict[predict_year].to_csv('michelin_{}_model_data.csv'.format(str(predict_year)))\n",
    "    \n",
    "\n",
    "# main_points = ['total_median', 'total_slope','div4_food_quality', 'div4_menu',  'div4_service', 'div4_value']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
